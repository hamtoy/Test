# mypy: allow-untyped-decorators
"""QA ìƒì„± ì—”ë“œí¬ì¸íŠ¸."""

from __future__ import annotations

import asyncio
import re
from datetime import datetime
from typing import Any, Dict, Optional, cast

from fastapi import APIRouter, HTTPException
from tenacity import retry, stop_after_attempt, wait_exponential

from checks.detect_forbidden_patterns import find_formatting_violations, find_violations
from src.agent import GeminiAgent
from src.config.constants import (
    DEFAULT_ANSWER_RULES,
    QA_BATCH_TYPES,
    QA_BATCH_TYPES_THREE,
    QA_GENERATION_OCR_TRUNCATE_LENGTH,
)
from src.config.exceptions import SafetyFilterError
from src.qa.rule_loader import RuleLoader
from src.qa.validator import UnifiedValidator, validate_constraints
from src.web.cache import answer_cache
from src.web.models import GenerateQARequest
from src.web.response import APIMetadata, build_response
from src.web.utils import QTYPE_MAP, load_ocr_text, postprocess_answer

from .qa_common import (
    _difficulty_hint,
    _get_agent,
    _get_config,
    _get_kg,
    _get_pipeline,
    _get_validator_class,
    get_cached_kg,
    logger,
)

router = APIRouter(prefix="/api", tags=["qa-generation"])


@router.get("/qa/cache/stats")
async def get_cache_stats() -> Dict[str, Any]:
    """Get cache statistics (PHASE 2B: Performance monitoring).
    
    Returns:
        Cache metrics including hit rate, size, and performance impact
    """
    stats = answer_cache.get_stats()
    # Add estimated time saved
    time_saved_seconds = stats["hits"] * 9  # Average 6-12s, use 9s as estimate
    stats["estimated_time_saved_seconds"] = time_saved_seconds
    stats["estimated_time_saved_minutes"] = round(time_saved_seconds / 60, 2)
    
    return {
        "success": True,
        "data": stats,
        "message": f"Cache hit rate: {stats['hit_rate_percent']:.1f}%",
    }


@router.post("/qa/cache/clear")
async def clear_cache() -> Dict[str, Any]:
    """Clear all cached answers (PHASE 2B: Cache management).
    
    Returns:
        Success message with number of entries cleared
    """
    size_before = answer_cache.get_stats()["cache_size"]
    answer_cache.clear()
    
    return {
        "success": True,
        "data": {"entries_cleared": size_before},
        "message": f"Cleared {size_before} cache entries",
    }


@router.post("/qa/generate")
async def api_generate_qa(body: GenerateQARequest) -> Dict[str, Any]:
    """QA ìƒì„± (ë°°ì¹˜: ì „ì²´ ì„¤ëª… ì„ í–‰ í›„ ë³‘ë ¬, ë‹¨ì¼: íƒ€ìž…ë³„ ìƒì„±)."""
    current_agent = _get_agent()
    if current_agent is None:
        raise HTTPException(status_code=500, detail="Agent ì´ˆê¸°í™” ì‹¤íŒ¨")

    ocr_text = body.ocr_text or load_ocr_text(_get_config())

    try:
        start = datetime.now()
        if body.mode in {"batch", "batch_three"}:
            results: list[Dict[str, Any]] = []

            batch_types = body.batch_types or QA_BATCH_TYPES
            if body.mode == "batch_three" and body.batch_types is None:
                batch_types = QA_BATCH_TYPES_THREE
            if not batch_types:
                raise HTTPException(
                    status_code=400, detail="batch_typesì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤."
                )

            first_type = batch_types[0]
            first_query: str = ""

            # 1ë‹¨ê³„: global_explanation ìˆœì°¨ ìƒì„±
            try:
                first_pair = await asyncio.wait_for(
                    generate_single_qa_with_retry(current_agent, ocr_text, first_type),
                    timeout=_get_config().qa_single_timeout,
                )
                results.append(first_pair)
                first_query = first_pair.get("query", "")
            except Exception as exc:  # noqa: BLE001
                logger.error("%s ìƒì„± ì‹¤íŒ¨: %s", first_type, exc)
                results.append(
                    {
                        "type": first_type,
                        "query": "ìƒì„± ì‹¤íŒ¨",
                        "answer": f"ì¼ì‹œì  ì˜¤ë¥˜: {str(exc)[:100]}",
                    }
                )

            # 2ë‹¨ê³„: ë‚˜ë¨¸ì§€ íƒ€ìž… ë³‘ë ¬ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš© previous_queries ì „ë‹¬)
            remaining_types = batch_types[1:]
            remaining_pairs = await asyncio.wait_for(
                asyncio.gather(
                    *[
                        generate_single_qa_with_retry(
                            current_agent,
                            ocr_text,
                            qtype,
                            previous_queries=[first_query] if first_query else None,
                        )
                        for qtype in remaining_types
                    ],
                    return_exceptions=True,
                ),
                timeout=_get_config().qa_batch_timeout,
            )

            for i, pair in enumerate(remaining_pairs):
                if isinstance(pair, Exception):
                    logger.error("%s ìƒì„± ì‹¤íŒ¨: %s", remaining_types[i], pair)
                    results.append(
                        {
                            "type": remaining_types[i],
                            "query": "ìƒì„± ì‹¤íŒ¨",
                            "answer": f"ì¼ì‹œì  ì˜¤ë¥˜: {str(pair)[:100]}",
                        }
                    )
                else:
                    results.append(cast(Dict[str, Any], pair))

            duration = (datetime.now() - start).total_seconds()
            meta = APIMetadata(duration=duration)
            return cast(
                Dict[str, Any],
                build_response(
                    {"mode": "batch", "pairs": results},
                    metadata=meta,
                    config=_get_config(),
                ),
            )

        if not body.qtype:
            raise HTTPException(status_code=400, detail="qtypeì´ í•„ìš”í•©ë‹ˆë‹¤.")
        pair = await asyncio.wait_for(
            generate_single_qa(current_agent, ocr_text, body.qtype),
            timeout=_get_config().qa_single_timeout,
        )
        duration = (datetime.now() - start).total_seconds()
        meta = APIMetadata(duration=duration)
        return cast(
            Dict[str, Any],
            build_response(
                {"mode": "single", "pair": pair},
                metadata=meta,
                config=_get_config(),
            ),
        )

    except asyncio.TimeoutError:
        timeout_msg = (
            f"ìƒì„± ì‹œê°„ ì´ˆê³¼ ({_get_config().qa_batch_timeout if body.mode == 'batch' else _get_config().qa_single_timeout}ì´ˆ). "
            "ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
        raise HTTPException(status_code=504, detail=timeout_msg)
    except Exception as e:
        logger.error("QA ìƒì„± ì‹¤íŒ¨: %s", e)
        raise HTTPException(status_code=500, detail=f"ìƒì„± ì‹¤íŒ¨: {str(e)}")


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    reraise=True,
)
async def generate_single_qa_with_retry(
    agent: GeminiAgent,
    ocr_text: str,
    qtype: str,
    previous_queries: Optional[list[str]] = None,
) -> Dict[str, Any]:
    """ìž¬ì‹œë„ ë¡œì§ì´ ìžˆëŠ” QA ìƒì„± ëž˜í¼."""
    return await generate_single_qa(agent, ocr_text, qtype, previous_queries)


async def generate_single_qa(
    agent: GeminiAgent,
    ocr_text: str,
    qtype: str,
    previous_queries: Optional[list[str]] = None,
) -> Dict[str, Any]:
    """ë‹¨ì¼ QA ìƒì„± - ê·œì¹™ ì ìš© ë³´ìž¥ + í˜¸ì¶œ ìµœì†Œí™”."""
    current_kg = _get_kg()
    current_pipeline = _get_pipeline()
    normalized_qtype = QTYPE_MAP.get(qtype, "explanation")
    
    # PHASE 2-1: Map globalexplanation to explanation for better rule coverage
    if qtype == "globalexplanation":
        normalized_qtype = "explanation"  # Use explanation rules for better quality
        logger.info(
            "Query type 'globalexplanation' normalized to 'explanation' for rule loading (quality improvement)"
        )
    else:
        logger.info(
            "Query type '%s' normalized to '%s' for rule loading",
            qtype,
            normalized_qtype,
        )
    
    query_intent = None

    if qtype == "target_short":
        query_intent = "ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸"
        if previous_queries:
            prev_text = "\n".join(f"- {q}" for q in previous_queries if q)
            query_intent += f"""

[ì¤‘ë³µ ë°©ì§€]
ë‹¤ìŒ ì§ˆì˜ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ êµ¬ì²´ì  íŒ©íŠ¸(ë‚ ì§œ, ìˆ˜ì¹˜, ëª…ì¹­ ë“±)ë¥¼ ì§ˆë¬¸í•˜ì„¸ìš”:
{prev_text}
"""
    elif qtype == "target_long":
        query_intent = "í•µì‹¬ ìš”ì ì„ ë¬»ëŠ” ì§ˆë¬¸"
        if previous_queries:
            prev_text = "\n".join(f"- {q}" for q in previous_queries if q)
            query_intent += f"""

[ì¤‘ë³µ ë°©ì§€]
ë‹¤ìŒ ì§ˆì˜ì™€ ë‹¤ë¥¸ ê´€ì /ì„¸ë¶€ í•­ëª©ì„ ë¬»ëŠ” ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:
{prev_text}
"""
    elif qtype == "reasoning":
        query_intent = "ì¶”ë¡ /ì˜ˆì¸¡ ì§ˆë¬¸"
    elif qtype == "global_explanation":
        query_intent = "ì „ì²´ ë‚´ìš© ì„¤ëª… ì§ˆë¬¸"

    # ì¤‘ë³µ/ë³‘ë ¬ ì§ˆë¬¸ ë°©ì§€ ê³µí†µ ì§€ì‹œ
    single_focus_clause = """
[ë‹¨ì¼ í¬ì»¤ìŠ¤ í•„ìˆ˜]
- í•œ ê°€ì§€ ê³¼ì—…ë§Œ ì§ˆë¬¸ (ê·¼ê±°+ì „ë§ì²˜ëŸ¼ ë‘ í•­ëª©ì„ ë™ì‹œì— ë¬»ì§€ ë§ ê²ƒ)
- 'ì™€/ê³¼/ë°/ë˜ëŠ”'ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ë³‘ë ¬ ì—°ê²° ê¸ˆì§€
- í•„ìš”í•˜ë©´ í•œ í•­ëª©ë§Œ ë¬»ë„ë¡ ìž¬ìž‘ì„±
"""
    if query_intent:
        query_intent += single_focus_clause
    else:
        query_intent = single_focus_clause

    rule_loader = RuleLoader(current_kg)
    rules_list = rule_loader.get_rules_for_type(normalized_qtype, DEFAULT_ANSWER_RULES)
    query_constraints: list[Dict[str, Any]] = []
    answer_constraints: list[Dict[str, Any]] = []
    formatting_rules: list[str] = []
    unified_validator = UnifiedValidator(current_kg, current_pipeline)
    kg_wrapper: Optional[Any] = get_cached_kg()

    if kg_wrapper is not None:
        try:
            # [Fix] Step 1: Enhanced type validation with detailed logging
            constraints = kg_wrapper.get_constraints_for_query_type(qtype)

            if not isinstance(constraints, list):
                logger.error(
                    "ðŸ”´ Invalid constraints type from Neo4j: expected list, got %s. Value: %r",
                    type(constraints).__name__,
                    repr(constraints)[:100],
                )
                constraints = []

            # [Fix] Step 2: Validate each item is a dict with detailed logging
            valid_constraints = []
            invalid_items = []

            for c in constraints:
                if isinstance(c, dict):
                    valid_constraints.append(c)
                else:
                    invalid_items.append(
                        {"type": type(c).__name__, "value": repr(c)[:50]}
                    )

            if invalid_items:
                logger.error(
                    "ðŸ”´ Invalid constraint items dropped: %d/%d. Samples: %s",
                    len(invalid_items),
                    len(constraints),
                    str(invalid_items[:3])[:200],  # Limit log message size
                )

            # [Fix] Step 3: Safe category access with .get()
            query_constraints = [
                c for c in valid_constraints if c.get("category") in ["query", "both"]
            ]
            answer_constraints = [
                c for c in valid_constraints if c.get("category") in ["answer", "both"]
            ]

            # Success logging
            logger.info(
                "âœ… Constraints loaded: query=%d, answer=%d",
                len(query_constraints),
                len(answer_constraints),
            )
            # [Fix] Step 4: Enhanced formatting rules validation
            try:
                fmt_rules = kg_wrapper.get_formatting_rules_for_query_type(
                    normalized_qtype
                )

                # Type validation with detailed logging
                if not isinstance(fmt_rules, list):
                    logger.error(
                        "ðŸ”´ Invalid formatting rules type: expected list, got %s",
                        type(fmt_rules).__name__,
                    )
                    fmt_rules = []

                # Validate each rule is a dict
                valid_fmt_rules = []
                for fr in fmt_rules:
                    if isinstance(fr, dict):
                        desc = fr.get("description") or fr.get("text")
                        if desc:
                            formatting_rules.append(desc)
                            valid_fmt_rules.append(fr)
                    else:
                        logger.warning(
                            "Invalid formatting rule (not dict): %s",
                            type(fr).__name__,
                        )

                logger.info("âœ… Formatting rules loaded: %d", len(formatting_rules))
            except Exception as e:  # noqa: BLE001
                logger.debug("ì„œì‹ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: %s", e)

            logger.info(
                "%s íƒ€ìž…: ì§ˆì˜ ì œì•½ %sê°œ, ë‹µë³€ ì œì•½ %sê°œ ì¡°íšŒ",
                qtype,
                len(query_constraints),
                len(answer_constraints),
            )
        except Exception as e:
            logger.warning("ê·œì¹™ ì¡°íšŒ ì‹¤íŒ¨: %s", e)

    # ì§ˆì˜ ì¤‘ë³µ/ë³µí•© ë°©ì§€ìš© ê³µí†µ ì œì•½ ì¶”ê°€
    query_constraints.append(
        {
            "description": "ë‹¨ì¼ ê³¼ì—…ë§Œ ë¬»ê¸°: 'ì™€/ê³¼/ë°/ë˜ëŠ”'ìœ¼ë¡œ ë³‘ë ¬ ì§ˆë¬¸(ë‘ ê°€ì§€ ì´ìƒ ìš”êµ¬) ê¸ˆì§€",
            "priority": 100,
            "category": "query",
        }
    )

    if not rules_list:
        rules_list = list(DEFAULT_ANSWER_RULES)
        logger.info("Neo4j ê·œì¹™ ì—†ìŒ, ê¸°ë³¸ ê·œì¹™ ì‚¬ìš©")

    extra_instructions = "ì§ˆì˜ ìœ í˜•ì— ë§žê²Œ ìž‘ì„±í•˜ì„¸ìš”."
    length_constraint = ""
    if normalized_qtype == "reasoning":
        extra_instructions = """ì¶”ë¡ í˜• ë‹µë³€ìž…ë‹ˆë‹¤.
- 'ê·¼ê±°', 'ì¶”ë¡  ê³¼ì •', 'ê²°ë¡ ' ë“± ëª…ì‹œì  ë¼ë²¨/ì†Œì œëª© ì ˆëŒ€ ê¸ˆì§€
- ì†Œì œëª©ì„ ì“°ë©´ ìžì—°ìŠ¤ëŸ¬ìš´ ì„œë¡ -ë³¸ë¡ -ê²°ë¡  íë¦„ë§Œ ìœ ì§€(í—¤ë”ë¡œ 'ì„œë¡ /ë³¸ë¡ /ê²°ë¡ ' ê¸ˆì§€)
- ë‘ê´„ì‹ìœ¼ë¡œ í•µì‹¬ ì „ë§ì„ ë¨¼ì € ì œì‹œ
- 'ì´ëŸ¬í•œ ë°°ê²½ì—ëŠ”', 'ì´ë¥¼ í†µí•´', 'ë”°ë¼ì„œ' ë“± ìžì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì–´ ì‚¬ìš©
- 'ìš”ì•½ë¬¸', 'ì •ë¦¬í•˜ë©´' ë“±ì˜ í—¤ë” ê¸ˆì§€"""
        length_constraint = """
[ë‹µë³€ í˜•ì‹]
ì¶”ë¡ í˜• ë‹µë³€ìž…ë‹ˆë‹¤.
- 'ìš”ì•½ë¬¸' ê°™ì€ í—¤ë” ì‚¬ìš© ê¸ˆì§€
- ê·¼ê±° 2~3ê°œì™€ ê²°ë¡ ì„ ëª…í™•ížˆ ì œì‹œ
"""
    elif normalized_qtype == "explanation":
        extra_instructions = """ì„¤ëª…í˜• ë‹µë³€ìž…ë‹ˆë‹¤.
- ì†Œì œëª©ì„ ì“¸ ë•ŒëŠ” ìžì—°ìŠ¤ëŸ¬ìš´ ì„œë¡ -ë³¸ë¡ -ê²°ë¡  íë¦„ ìœ ì§€ (í—¤ë”ì— 'ì„œë¡ /ë³¸ë¡ /ê²°ë¡ ' ì§ì ‘ í‘œê¸° ê¸ˆì§€)
- ë¶ˆí•„ìš”í•œ ë°˜ë³µ, ìž¥í™©í•œ ìˆ˜ì‹ì–´ ê¸ˆì§€"""
    elif normalized_qtype == "target":
        if qtype == "target_short":
            length_constraint = """
[CRITICAL - ê¸¸ì´ ì œì•½]
**ì ˆëŒ€ ê·œì¹™**: ë‹µë³€ì€ ë°˜ë“œì‹œ 1-2ë¬¸ìž¥ ì´ë‚´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.
- ìµœëŒ€ 50ë‹¨ì–´ ì´ë‚´
- í•µì‹¬ë§Œ ì¶”ì¶œ
- ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ê¸ˆì§€
- ì˜ˆì‹œ/ë¶€ì—° ì„¤ëª… ê¸ˆì§€
"""
            rules_list = rules_list[:3]
        elif qtype == "target_long":
            length_constraint = """
[CRITICAL - ê¸¸ì´ ì œì•½]
**ì ˆëŒ€ ê·œì¹™**: ë‹µë³€ì€ ë°˜ë“œì‹œ 3-4ë¬¸ìž¥ ì´ë‚´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.
- ìµœëŒ€ 100ë‹¨ì–´ ì´ë‚´
- í•µì‹¬ ìš”ì ë§Œ ê°„ê²°í•˜ê²Œ
- ë¶ˆí•„ìš”í•œ ë°˜ë³µ ê¸ˆì§€
- ì†Œì œëª© ì‚¬ìš© ì‹œ ìžì—°ìŠ¤ëŸ¬ìš´ íë¦„ë§Œ ìœ ì§€(í—¤ë”ì— 'ì„œë¡ /ë³¸ë¡ /ê²°ë¡ ' í‘œê¸° ê¸ˆì§€)
"""
            rules_list = rules_list[:5]

    # PHASE 2B: Check cache before generation to save ~6-12s
    cache_key_inputs = (ocr_text[:1000], qtype)  # Use truncated OCR for cache key
    
    try:
        queries = await agent.generate_query(
            ocr_text,
            user_intent=query_intent,
            query_type=qtype,
            kg=kg_wrapper or current_kg,
            constraints=query_constraints,
        )
        if not queries:
            raise ValueError("ì§ˆì˜ ìƒì„± ì‹¤íŒ¨")

        query = queries[0]

        # PHASE 2B: Check cache after query generation (query is part of cache key)
        cached_result = answer_cache.get(query, cache_key_inputs[0], cache_key_inputs[1])
        if cached_result is not None:
            logger.info(
                "Returning cached answer for query_type=%s (saved ~6-12s generation time)",
                qtype,
            )
            return cached_result

        truncated_ocr = ocr_text[:QA_GENERATION_OCR_TRUNCATE_LENGTH]
        rules_in_answer = "\n".join(f"- {r}" for r in rules_list)
        formatting_text = ""
        if formatting_rules:
            formatting_text = "\n[ì„œì‹ ê·œì¹™ - í•„ìˆ˜ ì¤€ìˆ˜]\n" + "\n".join(
                f"- {r}" for r in formatting_rules
            )

        # Add markdown usage policy based on qtype (Phase 1: IMPROVEMENTS.md)
        if normalized_qtype == "target":
            formatting_text += (
                "\n\n[ë§ˆí¬ë‹¤ìš´ ì‚¬ìš©]\n"
                "í‰ë¬¸ìœ¼ë¡œë§Œ ìž‘ì„±í•˜ì„¸ìš”. "
                "ë§ˆí¬ë‹¤ìš´(**bold**, *italic*, - ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. "
                "(â†’ í›„ì²˜ë¦¬ì—ì„œ ëª¨ë‘ ì œê±°ë©ë‹ˆë‹¤)"
            )
        elif normalized_qtype in {"explanation", "reasoning"}:
            formatting_text += (
                "\n\n[ë§ˆí¬ë‹¤ìš´ ì‚¬ìš©]\n"
                "ë‹¤ìŒ ë§ˆí¬ë‹¤ìš´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”:\n"
                "âœ“ ì†Œì œëª©: **í…ìŠ¤íŠ¸** (ì œëª©ì€ bold)\n"
                "âœ“ ëª©ë¡: - í•­ëª© (ë¶ˆë¦¿ í¬ì¸íŠ¸)\n"
                "âœ— ë³¸ë¬¸: í‰ë¬¸ë§Œ (ë§ˆí¬ë‹¤ìš´ ì œê±°)\n"
                "\nì˜ˆì‹œ:\n"
                "**ì£¼ìš” í¬ì¸íŠ¸**\n"
                "- ì²« ë²ˆì§¸: ì„¤ëª…\n"
                "- ë‘ ë²ˆì§¸: ì„¤ëª…\n"
                "ì¶”ê°€ ë‚´ìš©ì€ í‰ë¬¸ìœ¼ë¡œ ìž‘ì„±í•©ë‹ˆë‹¤."
            )

        constraints_text = ""
        if answer_constraints:

            def _priority_value(item: Dict[str, Any]) -> float:
                val = item.get("priority")
                return float(val) if isinstance(val, (int, float)) else 0.0

            answer_constraints.sort(key=_priority_value, reverse=True)
            constraints_text = "\n".join(
                f"[ìš°ì„ ìˆœìœ„ {c.get('priority', 0)}] {c.get('description', '')}"
                for c in answer_constraints
            )

            # Phase 3: Validate constraint conflicts (IMPROVEMENTS.md)
            # Extract max_length from length_constraint if present
            max_length_val: Optional[int] = None
            if "50ë‹¨ì–´" in length_constraint:
                max_length_val = 50
            elif "100ë‹¨ì–´" in length_constraint:
                max_length_val = 100
            elif "200ë‹¨ì–´" in length_constraint:
                max_length_val = 200
            elif "300ë‹¨ì–´" in length_constraint:
                max_length_val = 300

            # Check for paragraph constraints in answer_constraints
            # Note: This is a heuristic parser for constraint descriptions.
            # Expected format: "Xë¬¸ë‹¨, ê° Yë‹¨ì–´ ì´ìƒ" or similar
            min_per_para: Optional[int] = None
            num_paras: Optional[int] = None
            for constraint in answer_constraints:
                desc = constraint.get("description", "").lower()
                if "ë¬¸ë‹¨" in desc and "ë‹¨ì–´" in desc:
                    # Try to extract numbers from constraint description
                    numbers = re.findall(r"\d+", desc)
                    if len(numbers) >= 2:
                        # Heuristic: first number might be paragraph count, second might be words
                        try:
                            if "ê°" in desc or "ë‹¹" in desc:
                                num_paras = int(numbers[0])
                                min_per_para = int(numbers[1])
                        except (ValueError, IndexError):
                            pass

            if max_length_val:
                is_valid, validation_msg = validate_constraints(
                    qtype=normalized_qtype,
                    max_length=max_length_val,
                    min_per_paragraph=min_per_para,
                    num_paragraphs=num_paras,
                )
                if not is_valid:
                    logger.warning(
                        "âš ï¸ ì œì•½ ì¶©ëŒ ê°ì§€: %s (qtype=%s)",
                        validation_msg,
                        normalized_qtype,
                    )

        difficulty_text = _difficulty_hint(ocr_text)
        evidence_clause = "ìˆ«ìžÂ·ê³ ìœ ëª…ì‚¬ëŠ” OCRì— ë‚˜ì˜¨ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ìž¥ì„ 1ê°œ í¬í•¨í•˜ì„¸ìš”."

        # Phase 2: Add explicit priority hierarchy and conflict resolution (IMPROVEMENTS.md)
        markdown_rule = (
            "í‰ë¬¸ë§Œ (ë§ˆí¬ë‹¤ìš´ ì œê±°)"
            if normalized_qtype == "target"
            else "êµ¬ì¡°ë§Œ ë§ˆí¬ë‹¤ìš´(ì œëª©/ëª©ë¡), ë‚´ìš©ì€ í‰ë¬¸"
        )
        max_length_text = ""
        if "ìµœëŒ€ 50ë‹¨ì–´" in length_constraint:
            max_length_text = "50ë‹¨ì–´"
        elif "ìµœëŒ€ 100ë‹¨ì–´" in length_constraint:
            max_length_text = "100ë‹¨ì–´"
        elif "200ë‹¨ì–´" in length_constraint:
            max_length_text = "200ë‹¨ì–´"
        else:
            max_length_text = "[MAX_LENGTH]ë‹¨ì–´"

        priority_hierarchy = f"""
[PRIORITY HIERARCHY]
Priority 0 (CRITICAL):
- {normalized_qtype} íƒ€ìž…: {markdown_rule}

Priority 10 (HIGH):
- ìµœëŒ€ ê¸¸ì´: {max_length_text} ì´ë‚´
- ê¸¸ì´ ì œì•½ ìœ„ë°˜ì€ ë¶ˆê°€ëŠ¥

Priority 20 (MEDIUM):
- êµ¬ì¡°í™” í˜•ì‹: {formatting_text if formatting_text else "ê¸°ë³¸ ì„œì‹"}

Priority 30 (LOW):
- ì¶”ê°€ ì§€ì‹œ: {extra_instructions}

[CONFLICT RESOLUTION]
ë§Œì•½ ì—¬ëŸ¬ ì œì•½ì´ ì¶©ëŒí•œë‹¤ë©´:
â†’ Priority 0 > Priority 10 > Priority 20 > Priority 30

[REASONING BEFORE RESPONSE]
ì‘ë‹µí•˜ê¸° ì „ì— ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
1. í˜„ìž¬ qtypeì€ ë¬´ì—‡ì¸ê°€? â†’ ì˜¬ë°”ë¥¸ ë§ˆí¬ë‹¤ìš´ ê·œì¹™ í™•ì¸ (Priority 0)
2. ê¸¸ì´ ì œì•½ì€ ëª‡ ë‹¨ì–´ì¸ê°€? â†’ {max_length_text} ì´ë‚´ ìœ ì§€ (Priority 10)
3. êµ¬ì¡°í™” ë°©ì‹ì€? â†’ formatting_text ê·œì¹™ ì ìš© (Priority 20)
4. ì¶”ê°€ ìš”ì²­ì‚¬í•­ì€? â†’ extra_instructions ì¶”ê°€ ì²˜ë¦¬ (Priority 30)
"""

        answer_prompt = f"""{priority_hierarchy}

{length_constraint}

{formatting_text}

[ì œì•½ì‚¬í•­]
{constraints_text or rules_in_answer}

[ì§ˆì˜]: {query}

[OCR í…ìŠ¤íŠ¸]
{truncated_ocr}

ìœ„ ê¸¸ì´/í˜•ì‹ ì œì•½ê³¼ ê·œì¹™ì„ ì—„ê²©ížˆ ì¤€ìˆ˜í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
{difficulty_text}
{evidence_clause}
{extra_instructions}"""

        draft_answer = await agent.rewrite_best_answer(
            ocr_text=ocr_text,
            best_answer=answer_prompt,
            cached_content=None,
            query_type=normalized_qtype,
            kg=kg_wrapper or current_kg,
            constraints=answer_constraints,
            length_constraint=length_constraint,
        )
        if not draft_answer:
            raise SafetyFilterError("No text content in response.")

        # í†µí•© ê²€ì¦ìœ¼ë¡œ ìˆ˜ì§‘í•  ìœ„ë°˜/ê²½ê³  (ì§ˆì˜ í¬í•¨í•˜ì—¬ ê¸ˆì§€ íŒ¨í„´ ê²€ì¦ ê°•í™”)
        val_result = unified_validator.validate_all(
            draft_answer, normalized_qtype, query
        )
        all_issues: list[str] = []

        sentences = [
            s
            for s in draft_answer.replace("?", ".").replace("!", ".").split(".")
            if s.strip()
        ]
        sentence_count = len(sentences)
        if normalized_qtype == "target":
            if qtype == "target_short" and sentence_count > 2:
                all_issues.append(f"1-2ë¬¸ìž¥ìœ¼ë¡œ ì¶•ì†Œ í•„ìš” (í˜„ìž¬ {sentence_count}ë¬¸ìž¥)")
            elif qtype == "target_long" and sentence_count > 4:
                all_issues.append(f"3-4ë¬¸ìž¥ìœ¼ë¡œ ì¶•ì†Œ í•„ìš” (í˜„ìž¬ {sentence_count}ë¬¸ìž¥)")

        all_violations: list[str] = []
        if normalized_qtype == "reasoning" and (
            "ìš”ì•½ë¬¸" in draft_answer or "ìš”ì•½" in draft_answer.splitlines()[0]
        ):
            all_violations.append("summary_header_not_allowed")

        # Explicit rule compliance check when KG is available (for tests/validation)
        if kg_wrapper is not None:
            try:
                validator_cls = _get_validator_class()
                validator = validator_cls(kg_wrapper)
                rule_check = validator._check_rule_compliance(
                    draft_answer, normalized_qtype
                )
                score = rule_check.get("score")
                score_val = score if isinstance(score, (int, float)) else 1.0
                if rule_check.get("violations") and score_val < 0.3:
                    all_violations.extend(rule_check.get("violations", []))
            except Exception:
                pass

        # ê¸°ì¡´ íƒì§€ + í†µí•© ê²€ì¦ ë³‘í•©
        violations = find_violations(draft_answer)
        if violations:
            for v in violations:
                v_type = v["type"]
                if v_type.startswith("error_pattern:ì‹œì˜ì„±"):
                    continue
                all_violations.append(v_type)

        formatting_violations = find_formatting_violations(draft_answer)
        for fv in formatting_violations:
            if fv.get("severity") == "error":
                all_violations.append(fv["type"])
                logger.warning(
                    "ì„œì‹ ìœ„ë°˜ ê°ì§€: %s - '%s'", fv.get("description", ""), fv["match"]
                )

        if current_pipeline is not None:
            validation = current_pipeline.validate_output(
                normalized_qtype, draft_answer
            )
            if not validation.get("valid", True):
                all_violations.extend(validation.get("violations", []))
            missing_rules = validation.get("missing_rules_hint", [])
            if missing_rules:
                logger.info("ëˆ„ë½ ê°€ëŠ¥ì„± ìžˆëŠ” ê·œì¹™: %s", missing_rules)

        if val_result.has_errors():
            all_violations.extend(
                [v.get("type", "rule") for v in val_result.violations]
            )
        if val_result.warnings:
            all_issues.extend(val_result.warnings)

        if all_violations:
            all_issues.extend(all_violations[:3])

        if all_issues:
            combined_request = "; ".join(all_issues)
            logger.warning("ê²€ì¦ ì‹¤íŒ¨, ìž¬ìƒì„±: %s", combined_request)
            draft_answer = await agent.rewrite_best_answer(
                ocr_text=ocr_text,
                best_answer=draft_answer,
                edit_request=f"ë‹¤ìŒ ì‚¬í•­ ìˆ˜ì •: {combined_request}",
                cached_content=None,
                constraints=answer_constraints,
                length_constraint=length_constraint,
            )

        final_answer = postprocess_answer(draft_answer, qtype)

        # PHASE 2B: Store result in cache for future requests
        result = {"type": qtype, "query": query, "answer": final_answer}
        answer_cache.set(query, cache_key_inputs[0], cache_key_inputs[1], result)
        logger.debug("Cached answer for query_type=%s", qtype)

        return result
    except Exception as e:
        logger.error("QA ìƒì„± ì‹¤íŒ¨: %s", e)
        raise
