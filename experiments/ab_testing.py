"""
A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

í”„ë¡¬í”„íŠ¸, íŒŒë¼ë¯¸í„° ë³€ê²½ íš¨ê³¼ ì¸¡ì •ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass, field
from typing import Any, Callable, Coroutine, TypeAlias

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ íƒ€ì… ë³„ì¹­
# (test_data, config) -> Coroutine returning result dict
TestFunction: TypeAlias = Callable[
    [Any, dict[str, Any]], Coroutine[Any, Any, dict[str, Any]]
]


@dataclass
class Variant:
    """ì‹¤í—˜ ë³€í˜•

    Attributes:
        name: ë³€í˜• ì´ë¦„
        config: ë³€í˜•ì— ì ìš©í•  ì„¤ì •
        weight: íŠ¸ë˜í”½ ë°°ë¶„ ë¹„ìœ¨ (ê¸°ë³¸: 1.0)
    """

    name: str
    config: dict[str, Any]
    weight: float = 1.0


@dataclass
class ExperimentResult:
    """ì‹¤í—˜ ê²°ê³¼

    Attributes:
        variant_name: ë³€í˜• ì´ë¦„
        success: ì„±ê³µ ì—¬ë¶€
        latency_ms: ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        cost_usd: ë¹„ìš© (USD)
        quality_score: í’ˆì§ˆ ì ìˆ˜ (0-10)
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """

    variant_name: str
    success: bool
    latency_ms: float
    cost_usd: float
    quality_score: float
    metadata: dict[str, Any] = field(default_factory=dict)


class ABTest:
    """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°

    ì—¬ëŸ¬ ë³€í˜•ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    """

    def __init__(self, name: str, variants: list[Variant]):
        """A/B í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”

        Args:
            name: ì‹¤í—˜ ì´ë¦„
            variants: í…ŒìŠ¤íŠ¸í•  ë³€í˜• ë¦¬ìŠ¤íŠ¸
        """
        self.name = name
        self.variants = variants
        self.results: list[ExperimentResult] = []

    async def run(
        self,
        test_func: TestFunction,
        test_data: list[Any],
        runs_per_variant: int = 10,
    ) -> None:
        """ì‹¤í—˜ ì‹¤í–‰

        Args:
            test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (data, config) -> result
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            runs_per_variant: ë³€í˜•ë‹¹ ì‹¤í–‰ íšŸìˆ˜
        """
        print(f"ğŸ§ª ì‹¤í—˜ ì‹œì‘: {self.name}")
        print(f"  ë³€í˜•: {len(self.variants)}ê°œ")
        print(f"  ë°ì´í„°: {len(test_data)}ê°œ")
        print(f"  ë°˜ë³µ: {runs_per_variant}íšŒ/ë³€í˜•")

        for variant in self.variants:
            print(f"\nâ–¶ ë³€í˜•: {variant.name}")

            for i, data in enumerate(test_data[:runs_per_variant]):
                try:
                    start = time.time()
                    result = await test_func(data, variant.config)
                    latency = (time.time() - start) * 1000

                    self.results.append(
                        ExperimentResult(
                            variant_name=variant.name,
                            success=True,
                            latency_ms=latency,
                            cost_usd=result.get("cost", 0),
                            quality_score=result.get("quality", 0),
                            metadata=result,
                        )
                    )

                    print(f"  âœ“ {i + 1}/{runs_per_variant}")

                except Exception as e:
                    print(f"  âœ— {i + 1}/{runs_per_variant}: {e}")
                    self.results.append(
                        ExperimentResult(
                            variant_name=variant.name,
                            success=False,
                            latency_ms=0,
                            cost_usd=0,
                            quality_score=0,
                            metadata={"error": str(e)},
                        )
                    )

        self._print_summary()

    def _print_summary(self) -> None:
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)

        for variant in self.variants:
            variant_results = [
                r for r in self.results if r.variant_name == variant.name
            ]

            if not variant_results:
                continue

            success_count = sum(1 for r in variant_results if r.success)
            success_rate = success_count / len(variant_results)

            successful_results = [r for r in variant_results if r.success]

            if successful_results:
                avg_latency = sum(r.latency_ms for r in successful_results) / len(
                    successful_results
                )
                avg_cost = sum(r.cost_usd for r in successful_results) / len(
                    successful_results
                )
                avg_quality = sum(r.quality_score for r in successful_results) / len(
                    successful_results
                )
            else:
                avg_latency = 0
                avg_cost = 0
                avg_quality = 0

            print(f"\nğŸ”¹ {variant.name}")
            print(f"  ì„±ê³µë¥ : {success_rate * 100:.1f}%")
            print(f"  í‰ê·  ë ˆì´í„´ì‹œ: {avg_latency:.0f}ms")
            print(f"  í‰ê·  ë¹„ìš©: ${avg_cost:.4f}")
            print(f"  í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}/10")

    def get_best_variant(self, metric: str = "quality_score") -> str | None:
        """ìµœê³  ì„±ëŠ¥ ë³€í˜• ë°˜í™˜

        Args:
            metric: ë¹„êµ ê¸°ì¤€ (quality_score, latency_ms, cost_usd)

        Returns:
            ìµœê³  ì„±ëŠ¥ ë³€í˜• ì´ë¦„
        """
        variant_scores: dict[str, float] = {}

        for variant in self.variants:
            variant_results = [
                r for r in self.results if r.variant_name == variant.name and r.success
            ]

            if not variant_results:
                continue

            if metric == "latency_ms":
                # ë ˆì´í„´ì‹œëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ìŒìˆ˜ë¡œ ë³€í™˜)
                score = -sum(r.latency_ms for r in variant_results) / len(
                    variant_results
                )
            elif metric == "cost_usd":
                # ë¹„ìš©ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ìŒìˆ˜ë¡œ ë³€í™˜)
                score = -sum(r.cost_usd for r in variant_results) / len(variant_results)
            else:
                # í’ˆì§ˆì€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                score = sum(r.quality_score for r in variant_results) / len(
                    variant_results
                )

            variant_scores[variant.name] = score

        if not variant_scores:
            return None

        return max(variant_scores, key=lambda k: variant_scores[k])


def run_ab_test(
    name: str,
    variants: list[Variant],
    test_func: TestFunction,
    test_data: list[Any],
    runs_per_variant: int = 10,
) -> ABTest:
    """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í—¬í¼

    Args:
        name: ì‹¤í—˜ ì´ë¦„
        variants: í…ŒìŠ¤íŠ¸í•  ë³€í˜• ë¦¬ìŠ¤íŠ¸
        test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        runs_per_variant: ë³€í˜•ë‹¹ ì‹¤í–‰ íšŸìˆ˜

    Returns:
        ì™„ë£Œëœ ABTest ì¸ìŠ¤í„´ìŠ¤
    """
    ab_test = ABTest(name, variants)
    asyncio.run(ab_test.run(test_func, test_data, runs_per_variant))
    return ab_test


__all__ = [
    "Variant",
    "ExperimentResult",
    "ABTest",
    "run_ab_test",
    "TestFunction",
]
