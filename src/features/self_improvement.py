"""ìê°€ ê°œì„  ì‹œìŠ¤í…œ - Self-improvement system with performance trend analysis."""

from __future__ import annotations

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


class SelfImprovingSystem:
    """ìê°€ ê°œì„  ì‹œìŠ¤í…œ.

    Analyzes performance trends and generates improvement suggestions.
    """

    def __init__(
        self,
        history_file: Path | None = None,
        suggestions_file: Path | None = None,
    ) -> None:
        """Initialize the self-improving system.

        Args:
            history_file: Path to performance history JSONL file
            suggestions_file: Path to output suggestions JSON file
        """
        self.history_file = history_file or Path("data/performance_history.jsonl")
        self.suggestions_file = suggestions_file or Path(
            "reports/improvement_suggestions.json",
        )

    def _load_history(self, days: int = 30) -> list[dict[str, Any]]:
        """Load performance history for the specified number of days.

        Args:
            days: Number of days to look back

        Returns:
            List of history entry dictionaries
        """
        if not self.history_file.exists():
            return []

        cutoff = datetime.now() - timedelta(days=days)
        entries: list[dict[str, Any]] = []

        try:
            with open(self.history_file, encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                        ts_str = entry.get("timestamp", "")
                        if ts_str:
                            try:
                                ts = datetime.fromisoformat(
                                    ts_str.replace("Z", "+00:00"),
                                )
                                if ts.replace(tzinfo=None) >= cutoff:
                                    entries.append(entry)
                            except ValueError:
                                entries.append(entry)
                        else:
                            entries.append(entry)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            logger.warning(f"Failed to load history file: {e}")

        return entries

    def _mean(self, entries: list[dict[str, Any]], key: str) -> float:
        """Calculate mean of a numeric key in entries."""
        if not entries:
            return 0.0
        total = sum(float(e.get(key, 0) or 0.0) for e in entries)
        return total / len(entries)

    def _sum_metric(self, entries: list[dict[str, Any]], key: str) -> float:
        """Calculate sum of a numeric key in entries."""
        return float(sum(float(e.get(key, 0) or 0.0) for e in entries))

    def _append_issue(
        self,
        issues: list[dict[str, Any]],
        *,
        issue_type: str,
        severity: str,
        description: str,
        suggestions: list[str],
        auto_fix_available: bool,
        auto_fix_action: str | None = None,
    ) -> None:
        """Append an issue record to the list to avoid duplication."""
        issue: dict[str, Any] = {
            "type": issue_type,
            "severity": severity,
            "description": description,
            "suggestions": suggestions,
            "auto_fix_available": auto_fix_available,
        }
        if auto_fix_action:
            issue["auto_fix_action"] = auto_fix_action
        issues.append(issue)

    async def analyze_and_suggest(self) -> dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ë° ê°œì„  ì œì•ˆ.

        Analyze performance trends and generate improvement suggestions.

        Returns:
            Report dictionary containing issues and suggestions
        """
        # 1. ìµœê·¼ 30ì¼ ë°ì´í„° ë¡œë“œ
        history = self._load_history(days=30)

        if len(history) < 7:
            return {"status": "insufficient_data"}

        # 2. íŠ¸ë Œë“œ ë¶„ì„
        trends = self._analyze_trends(history)

        # 3. ë¬¸ì œ ê°ì§€
        issues: list[dict[str, Any]] = []

        # í’ˆì§ˆ ì €í•˜
        if trends["quality_declining"]:
            self._append_issue(
                issues,
                issue_type="quality_regression",
                severity="high",
                description="í’ˆì§ˆ ì ìˆ˜ê°€ ì§€ë‚œì£¼ ëŒ€ë¹„ 5% ì´ìƒ í•˜ë½",
                suggestions=[
                    "í”„ë¡¬í”„íŠ¸ ì¬ê²€í†  í•„ìš”",
                    "ëª¨ë¸ ì˜¨ë„ ì¡°ì • ê³ ë ¤ (í˜„ì¬: 0.2 â†’ 0.1)",
                    "ì˜ˆì‹œ ë°ì´í„° ì—…ë°ì´íŠ¸",
                ],
                auto_fix_available=False,
            )

        # ë¹„ìš© ì¦ê°€
        if trends["cost_increasing"]:
            self._append_issue(
                issues,
                issue_type="cost_spike",
                severity="medium",
                description=f"ë¹„ìš©ì´ {trends['cost_increase_percent']:.1f}% ì¦ê°€",
                suggestions=[
                    "ìºì‹± ì „ëµ ì¬ì¡°ì •",
                    f"í˜„ì¬ ìºì‹œ hit rate: {trends['cache_hit_rate']:.1f}% (ëª©í‘œ: 70%)",
                    "ë¶ˆí•„ìš”í•œ ì¬ìƒì„± ì¤„ì´ê¸°",
                ],
                auto_fix_available=True,
                auto_fix_action="adjust_cache_ttl",
            )

        # ë ˆì´í„´ì‹œ ì¦ê°€
        if trends["latency_increasing"]:
            self._append_issue(
                issues,
                issue_type="performance_degradation",
                severity="medium",
                description="í‰ê·  ë ˆì´í„´ì‹œ ì¦ê°€ ê°ì§€",
                suggestions=[
                    "Neo4j ì¸ë±ìŠ¤ í™•ì¸",
                    "Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸",
                    "ë™ì‹œì„± ì œí•œ ì¬ì¡°ì •",
                ],
                auto_fix_available=False,
            )

        # 4. ë¦¬í¬íŠ¸ ìƒì„±
        report: dict[str, Any] = {
            "timestamp": datetime.now().isoformat(),
            "analysis_period_days": 30,
            "issues_found": len(issues),
            "issues": issues,
            "trends": trends,
        }

        # ì €ì¥
        self.suggestions_file.parent.mkdir(parents=True, exist_ok=True)
        self.suggestions_file.write_text(
            json.dumps(report, indent=2, ensure_ascii=False),
            encoding="utf-8",
        )

        # 5. ìë™ ìˆ˜ì • ì‹¤í–‰ (ìŠ¹ì¸ëœ ê²½ìš°ë§Œ)
        if any(issue.get("auto_fix_available", False) for issue in issues):
            await self._apply_auto_fixes(issues)

        logger.info(f"ğŸ’¡ {len(issues)}ê°œ ê°œì„  ì œì•ˆ ìƒì„±: {self.suggestions_file}")

        return report

    def _analyze_trends(self, history: list[dict[str, Any]]) -> dict[str, Any]:
        """íŠ¸ë Œë“œ ê³„ì‚°.

        Calculate performance trends comparing recent vs previous periods.

        Args:
            history: List of history entry dictionaries

        Returns:
            Dictionary containing trend analysis
        """
        # ìµœê·¼ 7ì¼ vs ì´ì „ 7ì¼
        recent = history[-7:] if len(history) >= 7 else history
        previous = history[-14:-7] if len(history) >= 14 else history[: -len(recent)]

        # Avoid division by zero
        if not previous:
            previous = recent

        recent_quality = self._mean(recent, "quality")
        prev_quality = self._mean(previous, "quality")

        recent_cost = self._sum_metric(recent, "cost")
        prev_cost = self._sum_metric(previous, "cost")

        recent_latency = self._mean(recent, "latency")
        prev_latency = self._mean(previous, "latency")

        cache_hit_rate = self._mean(recent, "cache_hit_rate")

        # Calculate cost increase percentage safely
        cost_increase_percent = (
            ((recent_cost / prev_cost) - 1) * 100 if prev_cost > 0 else 0
        )

        return {
            "quality_declining": recent_quality < prev_quality * 0.95
            if prev_quality > 0
            else False,
            "quality_score": recent_quality,
            "quality_change": recent_quality - prev_quality,
            "cost_increasing": recent_cost > prev_cost * 1.2
            if prev_cost > 0
            else False,
            "cost_increase_percent": cost_increase_percent,
            "latency_increasing": recent_latency > prev_latency * 1.15
            if prev_latency > 0
            else False,
            "avg_latency_ms": recent_latency,
            "cache_hit_rate": cache_hit_rate,
        }

    async def _apply_auto_fixes(self, issues: list[dict[str, Any]]) -> None:
        """ìë™ ìˆ˜ì • ì ìš©.

        Apply automatic fixes for issues that support it.

        Args:
            issues: List of issue dictionaries
        """
        for issue in issues:
            if not issue.get("auto_fix_available", False):
                continue

            action = issue.get("auto_fix_action", "")

            if action == "adjust_cache_ttl":
                # ìºì‹œ TTL ìë™ ì¡°ì •
                logger.info("ğŸ”§ ìºì‹œ TTL ìë™ ì¡°ì • ì¤‘...")
                await self._adjust_cache_ttl()
                logger.info("   âœ“ TTL ì¦ê°€: 900s â†’ 1800s")

    async def _adjust_cache_ttl(self) -> None:
        """Adjust cache TTL settings.

        This is a placeholder for actual cache TTL adjustment logic.
        In a real implementation, this would update cache configuration.
        """
        # Placeholder - in production this would update actual cache settings
        logger.debug("Cache TTL adjustment placeholder")

    def send_slack_notification(self, report: dict[str, Any]) -> None:
        """ì¤‘ìš” ì´ìŠˆ ë°œìƒ ì‹œ Slack ì•Œë¦¼.

        Send Slack notification for high-severity issues.

        Args:
            report: Report dictionary containing issues
        """
        issues = report.get("issues", [])
        high_severity_issues = [i for i in issues if i.get("severity") == "high"]

        if not high_severity_issues:
            return

        message = f"âš ï¸ {len(high_severity_issues)}ê°œ ê³ ì‹¬ê°ë„ ì´ìŠˆ ê°ì§€\n"
        for issue in high_severity_issues:
            message += f"â€¢ {issue.get('description', 'Unknown issue')}\n"

        # Slack webhook í˜¸ì¶œ (placeholder)
        # In production: requests.post(SLACK_WEBHOOK_URL, json={"text": message})
        logger.info(f"Slack notification (placeholder): {message}")
