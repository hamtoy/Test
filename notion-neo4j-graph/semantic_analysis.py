"""Semantic Analysis module."""

import logging
import os
import re
import sys
from collections import Counter as CounterClass
from typing import Dict, List

from dotenv import load_dotenv
from neo4j import GraphDatabase

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("SemanticAnalysis")

load_dotenv()


class TextProcessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ ì¶”ì¶œ."""

    # ë¶ˆìš©ì–´ ëª©ë¡ (í•œêµ­ì–´/ì˜ì–´)
    STOPWORDS = {
        # í•œêµ­ì–´
        "ì´",
        "ê·¸",
        "ì €",
        "ê²ƒ",
        "ìˆ˜",
        "ë“±",
        "ë¥¼",
        "ì„",
        "ì€",
        "ëŠ”",
        "ê°€",
        "ì´",
        "ë„",
        "ì—",
        "ì˜",
        "ë¡œ",
        "í•œ",
        "í•˜ë‹¤",
        "ìˆë‹¤",
        "ì—†ë‹¤",
        "ë˜ë‹¤",
        "ì•Šë‹¤",
        "ê°™ë‹¤",
        "í•´ì„œ",
        "ìˆëŠ”",
        "í•˜ëŠ”",
        "ë°",
        "ë˜ëŠ”",
        "í•©ë‹ˆë‹¤",
        "ì…ë‹ˆë‹¤",
        "ìˆëŠ”",
        "ì—†ëŠ”",
        "ëŒ€í•œ",
        "ìœ„í•´",
        "í†µí•´",
        "ë”°ë¼",
        "ê²½ìš°",
        "ë•Œë¬¸",
        # ì˜ì–´
        "the",
        "a",
        "an",
        "in",
        "on",
        "at",
        "to",
        "for",
        "of",
        "and",
        "or",
        "is",
        "are",
        "was",
        "were",
        "be",
        "been",
        "being",
        "have",
        "has",
        "had",
        "do",
        "does",
        "did",
        "but",
        "if",
        "so",
        "not",
        "no",
        "can",
        "could",
        "will",
        "would",
        "should",
        "may",
        "might",
        "must",
        "this",
        "that",
        "it",
        "they",
        "we",
        "you",
        "he",
        "she",
        "what",
        "which",
        "who",
        "whom",
        "whose",
        "where",
        "when",
        "why",
        "how",
        "http",
        "https",
        "www",
        "com",
        "org",
        "net",
    }

    @staticmethod
    def normalize(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”: ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°."""
        # URL ì œê±°
        text = re.sub(r"https?://\S+|www\.\S+", "", text)
        # íŠ¹ìˆ˜ë¬¸ì ë° ìˆ«ì ì œê±° (í•œê¸€, ì˜ë¬¸, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r"[^ê°€-í£a-zA-Z\s]", " ", text)
        return text.lower().strip()

    @classmethod
    def extract_keywords(cls, text: str, top_n: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ."""
        normalized = cls.normalize(text)
        words = normalized.split()

        # ë¶ˆìš©ì–´ í•„í„°ë§ ë° ê¸¸ì´ ì œí•œ (2ê¸€ì ì´ìƒ)
        valid_words = [w for w in words if w not in cls.STOPWORDS and len(w) >= 2]

        # ë¹ˆë„ ë¶„ì„
        counter = CounterClass(valid_words)
        return [word for word, _ in counter.most_common(top_n)]


class SemanticAnalyzer:
    """Neo4j ë°ì´í„° ì˜ë¯¸ ë¶„ì„ê¸°."""

    BATCH_SIZE = 500

    def __init__(self):
        """ì´ˆê¸°í™”: í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ë° Neo4j ë“œë¼ì´ë²„ ì„¤ì •."""
        self._validate_env()
        self.driver = GraphDatabase.driver(
            os.environ["NEO4J_URI"],
            auth=(os.environ.get("NEO4J_USER", "neo4j"), os.environ["NEO4J_PASSWORD"]),
        )

    def _validate_env(self):
        """í™˜ê²½ ë³€ìˆ˜ ê²€ì¦."""
        required = ["NEO4J_URI", "NEO4J_PASSWORD"]
        missing = [key for key in required if not os.environ.get(key)]
        if missing:
            logger.error(f"âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing)}")
            sys.exit(1)

    def close(self):
        """Neo4j ë“œë¼ì´ë²„ ì¢…ë£Œ."""
        self.driver.close()

    def analyze_blocks(self):
        """ë¸”ë¡ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í‚¤ì›Œë“œ ë¶„ì„ í›„ Topic ì—°ê²°."""
        logger.info("ğŸ” ë¸”ë¡ ë°ì´í„° ë¶„ì„ ì‹œì‘...")

        try:
            with self.driver.session() as session:
                # 1. ëª¨ë“  ë¸”ë¡ì˜ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
                result = session.run("""
                    MATCH (b:Block)
                    WHERE b.content IS NOT NULL AND b.content <> ''
                    RETURN b.id AS id, b.content AS content
                """)

                blocks = list(result)
                logger.info(f"   - ë¶„ì„ ëŒ€ìƒ ë¸”ë¡: {len(blocks)}ê°œ")

                topic_mappings = []
                all_keywords = CounterClass()

                # 2. Pythonì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶€í•˜ ë¶„ì‚°)
                for record in blocks:
                    keywords = TextProcessor.extract_keywords(record["content"])
                    if keywords:
                        all_keywords.update(keywords)
                        topic_mappings.extend(
                            {"block_id": record["id"], "topic": kw} for kw in keywords
                        )

                logger.info(f"   - ì¶”ì¶œëœ ê³ ìœ  í‚¤ì›Œë“œ: {len(all_keywords)}ê°œ")

                # 3. ìƒìœ„ í‚¤ì›Œë“œë§Œ í•„í„°ë§ (ë…¸ì´ì¦ˆ ì œê±°)
                # ì „ì²´ ë¬¸ì„œì—ì„œ ìµœì†Œ 2íšŒ ì´ìƒ ë“±ì¥í•œ í‚¤ì›Œë“œë§Œ Topicìœ¼ë¡œ ìƒì„±
                valid_topics = {kw for kw, count in all_keywords.items() if count >= 2}

                final_mappings = [
                    m for m in topic_mappings if m["topic"] in valid_topics
                ]

                logger.info(
                    f"   - í•„í„°ë§ í›„ ë§¤í•‘: {len(final_mappings)}ê°œ (ìµœì†Œ ë¹ˆë„ 2íšŒ ì´ìƒ)"
                )

                # 4. ë°°ì¹˜ ë‹¨ìœ„ë¡œ Neo4j ì—…ë°ì´íŠ¸
                self._batch_update_topics(session, final_mappings)

        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _batch_update_topics(self, session, mappings: List[Dict]):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ Topic ë…¸ë“œ ìƒì„± ë° ì—°ê²°."""
        total = len(mappings)
        for i in range(0, total, self.BATCH_SIZE):
            batch = mappings[i : i + self.BATCH_SIZE]

            # Topic ë…¸ë“œ ìƒì„± ë° ê´€ê³„ ì„¤ì • (Optimized Cypher)
            query = """
            UNWIND $batch AS item
            MERGE (t:Topic {name: item.topic})
            WITH t, item
            MATCH (b:Block {id: item.block_id})
            MERGE (b)-[:DISCUSSES]->(t)
            """

            session.run(query, batch=batch)
            logger.info(
                f"   - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... ({min(i + self.BATCH_SIZE, total)}/{total})"
            )

        logger.info("âœ… Topic ìƒì„± ë° ì—°ê²° ì™„ë£Œ")


def main():
    """ì˜ë¯¸ ë¶„ì„ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    analyzer = SemanticAnalyzer()
    try:
        analyzer.analyze_blocks()
    finally:
        analyzer.close()


if __name__ == "__main__":
    main()
